[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?\n\n\n\n\n\n\n\n\nJan 17, 2026\n\n\nTeem KWONG\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post1.html",
    "href": "posts/post1.html",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "",
    "text": "In the early days of Large Language Model (LLM) adoption, users often treated models like GPT-3 or early Gemini versions as sophisticated search engines. We provided a query, and the model returned a result. This works perfectly for retrieving facts: “What is the boiling point of nitrogen?” results in a quick, accurate answer.\nHowever, as we moved toward more complex tasks, like solving word problems or planning a multi-step schedule, the “Black Box” nature of LLMs became a liability. Imagine asking the model: “If I buy 3 apples for $2 each and a $10 cake, how much change do I get from $20?” A model might confidently blurt out “$8” because it recognized the numbers with dollar sign but “hallucinated” a shortcut, completely forgetting 3 apples. Without seeing the steps, you have no idea where the math went wrong. It either gives you the right destination or leaves you stranded in the wrong place with no map in between.\nThe breakthrough came when researchers realized that LLMs, much like human students, perform significantly better when they are forced to “show their work.” This technique, known as Chain-of-Thought (CoT) Prompting, is not just a trick of language, it is a fundamental shift in how we interact with neural networks."
  },
  {
    "objectID": "posts/post1.html#introduction-the-black-box-problem",
    "href": "posts/post1.html#introduction-the-black-box-problem",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "",
    "text": "In the early days of Large Language Model (LLM) adoption, users often treated models like GPT-3 or early Gemini versions as sophisticated search engines. We provided a query, and the model returned a result. This works perfectly for retrieving facts: “What is the boiling point of nitrogen?” results in a quick, accurate answer.\nHowever, as we moved toward more complex tasks, like solving word problems or planning a multi-step schedule, the “Black Box” nature of LLMs became a liability. Imagine asking the model: “If I buy 3 apples for $2 each and a $10 cake, how much change do I get from $20?” A model might confidently blurt out “$8” because it recognized the numbers with dollar sign but “hallucinated” a shortcut, completely forgetting 3 apples. Without seeing the steps, you have no idea where the math went wrong. It either gives you the right destination or leaves you stranded in the wrong place with no map in between.\nThe breakthrough came when researchers realized that LLMs, much like human students, perform significantly better when they are forced to “show their work.” This technique, known as Chain-of-Thought (CoT) Prompting, is not just a trick of language, it is a fundamental shift in how we interact with neural networks."
  },
  {
    "objectID": "posts/post1.html#what-is-chain-of-thought-cot-prompting",
    "href": "posts/post1.html#what-is-chain-of-thought-cot-prompting",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "What is Chain-of-Thought (CoT) Prompting?",
    "text": "What is Chain-of-Thought (CoT) Prompting?\nChain-of-Thought prompting is a technique where we ask an AI model to explain its reasoning before giving the answer. Instead of jumping to conclusions, the model unfolds its “thought process” in natural language.\n\nA Comparative Example\nConsider the following logic puzzle: &gt; Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nStandard Prompting Output: &gt; Answer: 7. (Incorrect)\nCoT Prompting Output: &gt;\nStep 1: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls.\nStep 2: 5 + 6 = 11.\nStep 3: The answer is 11.\nAnswer: 11. (Incorrect)\nIn this case, the model followed the logical flow. By generating the text for Step 1, the model created a “new” context for Step 2. It no longer had to remember 2 cans of 3 tennis balls; it only had to focus on the result “6” that was now physically present in its recent memory.\n\nImage Source: Wei et al. (2022)"
  },
  {
    "objectID": "posts/post1.html#the-technical-why-building-logic-brick-by-brick",
    "href": "posts/post1.html#the-technical-why-building-logic-brick-by-brick",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "The Technical “Why”: Building Logic Brick by Brick",
    "text": "The Technical “Why”: Building Logic Brick by Brick\nTo understand why CoT works, we have to look under the hood of how Large Language Models process information. LLMs predict the next word (or “token”) based on everything that came before it. In a standard prompt, the model tries to leap from the question to the final answer in one go. This is where logic often breaks down.\n\n1. Logic Decomposition: The LEGO Principle\nCoT prompting works because it encourages logic decomposition—breaking a massive, intimidating problem into smaller, manageable parts. Think of it like building a complex LEGO castle. You wouldn’t try to manifest the entire castle at once; you’d follow the manual to build the baseplate, then the walls, then the towers.\nThis “divide and conquer” approach is essential for solving long word problems, planning detailed essays, or diagnosing software bugs. By constructing these smaller “logical blocks” first, the model can eventually snap them all together to form a complete, sturdy structure.\n\n\n2. Externalizing “Working Memory”\nIn a standard prompt, the model has to perform all its “thinking” inside its hidden layers before it types a single letter. This is like trying to solve a 10-digit multiplication problem entirely in your head without a piece of paper. You’re likely to lose track of a carry-over digit and fail.\nWhen a model writes down a reasoning step, it’s like placing a LEGO brick firmly into the baseplate. Once that brick is placed, the model can “see” it. It no longer has to use its internal “mental energy” to remember that specific sub-step because it is now part of the written text. This process acts as external working memory, allowing the model to “read” its own previous logic to inform what comes next.\n\n\n3. Error Correction: Catching the “Misplaced Brick”\nOne of the greatest strengths of the LEGO approach is error locality. If you are building a LEGO tower and you put a 2x4 brick where a 2x2 brick should be, you will notice the misalignment immediately as you try to add the next layer.\nSimilarly, when an LLM explains its intermediate reasoning, it is much easier to catch mistakes early. If the model realizes in Step 2 that its math doesn’t add up, the “physical” presence of that error in the text window often nudges the model to self-correct before it reaches the final answer. This leads to far more consistent and accurate results than a single “black box” guess."
  },
  {
    "objectID": "posts/post1.html#variations-of-the-cot-pattern",
    "href": "posts/post1.html#variations-of-the-cot-pattern",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "Variations of the CoT Pattern",
    "text": "Variations of the CoT Pattern\nAs the field of Prompt Engineering has matured, three primary variations of CoT have emerged as industry standards.\n\n1. Zero-Shot CoT: The “Magic” Instruction\nHere, you simply add a phrase like “Let’s think step by step” to a question. The model interprets this as an instruction to explain its reasoning, i.e. no examples needed.\n\nImage Source: Kojima et al. (2022)\n\n\n2. Few-Shot CoT: Exemplar-Based Reasoning\nThis version provides a few examples of problems and their worked-out solutions before the new question. It’s like giving a student a few solved examples before letting them try on their own.\n\n\n3. Self-Consistency: Voting for Accuracy\nSelf-consistency: Instead of relying on a single chain of thought, the model generates multiple reasoning paths, then picks the most consistent conclusion. This reduces random errors and produces more reliable results."
  },
  {
    "objectID": "posts/post1.html#practical-case-study-automating-data-cleaning",
    "href": "posts/post1.html#practical-case-study-automating-data-cleaning",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "Practical Case Study: Automating Data Cleaning",
    "text": "Practical Case Study: Automating Data Cleaning\nLet’s look at a real-world Data Science application. Imagine you have a messy dataset and you need an LLM to write a Python script to clean it.\nStandard Prompt: &gt; “Write a Python script to clean this CSV where ‘Date’ is in various formats and ‘Income’ has currency symbols.”\nThe model might write a quick script that misses certain edge cases, like “€” versus “$”.\nCoT Prompt: &gt; “Let’s approach this data cleaning task systematically. First, identify all possible date formats. Second, determine the best regex for stripping currency symbols. Third, handle missing values. Finally, write the script.”\nBy following this chain, the model is much more likely to produce robust code that accounts for the nuances of the data, as it has “thought through” the edge cases before it even began writing the first line of import pandas as pd."
  },
  {
    "objectID": "posts/post1.html#advantages-and-limitations",
    "href": "posts/post1.html#advantages-and-limitations",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nThe Benefits\n\nEnhanced Logical Transparency: CoT transforms the AI from a “black box” into an open book. Especially in complex fields like medical diagnostics or financial forecasting, it provides a clear reasoning chain of logic. Instead of just receiving a final number or statement, you can see exactly how the AI arrived at its conclusion.\nPrecision in Complex Reasoning: This approach is most powerful when tackling mathematical problems, strategic planning, or multi-step logic. By breaking down the process, the model stays “on track” during long-form generation, ensuring that the final output is grounded in a consistent, step-by-step methodology.\nEducational Insight: Beyond simply providing an answer, CoT serves as an instructional tool. It teaches the user the underlying methodology, showing the “how” and “why” behind a solution. This makes it an invaluable resource for learning how to approach similar strategic or logical challenges in the future.\n\n\n\nThe Trade-offs\n\nIncreased Resource Overhead: The primary cost of detailed reasoning is a higher token count. Because CoT generates full reasoning chains rather than just a final result, it consumes significantly more computational power.\nLatency & Speed Constraints: In environments where “speed is king,” CoT can be a bottleneck. Generating several paragraphs of logic takes more time than a concise one-sentence answer. This makes it unsuitable for real-time chatbots or simple queries, like weather updates or basic definitions, where the extra “thinking time” creates unnecessary delays for the user."
  },
  {
    "objectID": "posts/post1.html#conclusion-the-path-forward",
    "href": "posts/post1.html#conclusion-the-path-forward",
    "title": "How Does Chain-of-Thought (CoT) Prompting Influence the Outputs of Large Language Models (LLMs)?",
    "section": "Conclusion: The Path Forward",
    "text": "Conclusion: The Path Forward\nChain-of-Thought (CoT) prompting marks a fundamental shift in AI, evolving Large Language Models from simple “answer machines” into sophisticated reasoning engines. By making the model’s logic transparent and interpretable, CoT builds the necessary trust for AI integration in professional and academic environments.\nFor data scientists and enthusiasts alike, mastering CoT is essential. It serves as the bridge between superficial fluency and true logical depth, ensuring that the AI of the future is not only more accurate but also more transparent and reliable.\n\n\nReferences\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916\nChain-of-Thought (CoT) Prompting. (n.d.). Prompting Guide. Retrieved January 7, 2026, from https://www.promptingguide.ai/techniques/cot/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Teem, a Computer Science graduate with experience in IT support, system administration, and data analysis. My career goal is to leverage data-driven approaches to solve complex challenges and facilitate decision-making processes."
  },
  {
    "objectID": "slides.html#land-acknowledgement",
    "href": "slides.html#land-acknowledgement",
    "title": "Breaking the Black Box",
    "section": "Land Acknowledgement",
    "text": "Land Acknowledgement\nI would like to begin by acknowledging that we are on the traditional, ancestral, and unceded territory of the xʷməθkʷəy̓əm (Musqueam), Sḵwx̱wú7mesh (Squamish), and Tsleil-Waututh peoples. I am thankful to have the opportunity to live and learn on this land."
  },
  {
    "objectID": "slides.html#the-black-box-problem",
    "href": "slides.html#the-black-box-problem",
    "title": "Breaking the Black Box",
    "section": "The “Black Box” Problem",
    "text": "The “Black Box” Problem\nWhy standard LLMs fail complex tasks\n\nEarly users treated LLMs like search engine.\nModels are great at facts, but struggle with multi-step reasoning.\nModels return a wrong answer without reasoning."
  },
  {
    "objectID": "slides.html#what-is-chain-of-thought-cot",
    "href": "slides.html#what-is-chain-of-thought-cot",
    "title": "Breaking the Black Box",
    "section": "What is Chain-of-Thought (CoT)?",
    "text": "What is Chain-of-Thought (CoT)?\n\nA technique where we ask an AI model to explain its reasoning before giving the answer.\nInstead of jumping to conclusions, the model unfolds its “thought process”."
  },
  {
    "objectID": "slides.html#example",
    "href": "slides.html#example",
    "title": "Breaking the Black Box",
    "section": "Example",
    "text": "Example\nQuestion: Roger has 5 balls. He buys 2 cans (3 balls each). Total?\n\n\n\nStandard Prompting\nChain-of-Thought Prompting\n\n\n\n\nOutput: “7”\nStep 1: 5 balls.\n\n\nResult: Incorrect\nStep 2: (2 cans × 3) = 6 balls more.\n\n\n\nStep 3: 5 + 6 = 11.\n\n\n\nAnswer: 11 (Correct)\n\n\n\nWhy? The model uses its own output as “new context” for the next step."
  },
  {
    "objectID": "slides.html#the-lego-principle",
    "href": "slides.html#the-lego-principle",
    "title": "Breaking the Black Box",
    "section": "The LEGO Principle",
    "text": "The LEGO Principle\n1. Logic Decomposition\n\nLEGO castle\nMassive problems into smaller and manageable “blocks”.\nBuild the baseplate \\(\\rightarrow\\) Walls \\(\\rightarrow\\) Towers."
  },
  {
    "objectID": "slides.html#variations-of-the-cot-pattern",
    "href": "slides.html#variations-of-the-cot-pattern",
    "title": "Breaking the Black Box",
    "section": "Variations of the CoT Pattern",
    "text": "Variations of the CoT Pattern\n1. Zero-Shot CoT\nThe “Magic” Phrase: “Let’s think step by step.” No examples required.\n\nImage Source: Kojima et al. (2022)"
  },
  {
    "objectID": "slides.html#variations-of-the-cot-pattern-1",
    "href": "slides.html#variations-of-the-cot-pattern-1",
    "title": "Breaking the Black Box",
    "section": "Variations of the CoT Pattern",
    "text": "Variations of the CoT Pattern\n2. Few-Shot CoT\nProviding Exemplars. Showing the model 2-3 solved problems with worked-out logic.\n\nImage Source: Kojima et al. (2022)"
  },
  {
    "objectID": "slides.html#variations-of-the-cot-pattern-2",
    "href": "slides.html#variations-of-the-cot-pattern-2",
    "title": "Breaking the Black Box",
    "section": "Variations of the CoT Pattern",
    "text": "Variations of the CoT Pattern\n3. Self-Consistency\nThe “Majority Vote.” Generate 5 different paths; if 4 lead to “11” and 1 leads to “7”, choose 11.\n\nImage Source: Wang et al., 2022"
  },
  {
    "objectID": "slides.html#advantages-trade-offs",
    "href": "slides.html#advantages-trade-offs",
    "title": "Breaking the Black Box",
    "section": "Advantages & Trade-offs",
    "text": "Advantages & Trade-offs\nThe Benefits\n\nTransparency: Essential for Medical Diagnosis and Financial Decisions.\nPrecision: Stays “on track” during long generations."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Breaking the Black Box",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nFrom Answer Machines to Reasoning Engines.\nMany modern LLMs adopt this technique."
  },
  {
    "objectID": "slides.html#thank-you",
    "href": "slides.html#thank-you",
    "title": "Breaking the Black Box",
    "section": "Thank you!",
    "text": "Thank you!\nReferences:\n\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903.\nChain-of-Thought (CoT) Prompting. (n.d.). Prompting Guide. Retrieved January 7, 2026, from https://www.promptingguide.ai/techniques/cot/"
  }
]